---
title: "Bank Customer Targeting"
path: blog/cgiar-bank-customer-targeting
tags: [python, work]
date: 2020-12-17
cover: ./cover.jpg
excerpt: SIGNATEで開催された銀行の顧客ターゲティングコンペの参加記録です。
---

<p style="color: crimson">以下はただの日記と感想です。</p>

## 顧客ターゲティングコンペ
---
![score](https://user-images.githubusercontent.com/43092452/116044471-ec5bf500-a6ab-11eb-9379-de4658aaf8b4.jpg)

19 / 558 位

## コンペの課題
---
顧客属性データおよび、過去のキャンペーンでの接触情報に基づいて口座を開設したかを予測するモデルの構築。

評価関数はAUC。
## データセット
---
unknownが含まれているカラムが複数。Nanとして処理されているものがないので、使おうと思えばそのままモデルに投入できるような見た目的にはきれいなデータ。

ただ、コンペ終了後のフォーラムにも書かれていたが、オリジナルデータと比べて分布が大きく異なっている部分があった。
## コンペ中の日記
---
上にも書いたとおり、データセットがオリジナルのものと比べて異なった特徴を持っているものとなっていた。したがって、balanceなどを範囲ごとにグールプ化してみたり、新しい特徴量をいくつか作ったが、スコア改善にはほぼ貢献しなかった。unknownが含まれているカラムについては、データが不均衡なのに加えて、すでに加工されたデータセットなので補完することが難しかったので、そのままunknownとして扱った。

したがって、コンペの前半で特徴量作成の作業をストップし、各特徴量をそのままモデルで扱えるようにし、モデルの改良に注力した。モデルはRandomForest、XGBoost、LightGBM、CatBoostの採用を決め、アンサンブルを行って予測を出すことにした。Optunaでパラメーター調整を行い、このコンペの1つの壁となっていた0.85をこの段階で超えることができた。

次に、モデルが決定木ベースのものばかりだったため、NNを使用したモデルを作成。NNモデルは精度が0.83くらいで、すでにあるモデルと比較して精度が0.01程度低かったが、NNモデルを追加してアンサンブルを行うことで精度が向上した。ただ、最終的には公式のベンチマークを超えていたXGB、LGB、CBの3つのモデルからLGBをgbdtとdartに分けた4モデルを採用した。

最後に、採用した4モデルの訓練データに対する予測値を入力とし、正解ラベルを予測するロジスティック回帰モデルを作成した。そして、このモデルを加えた5つのモデルの予測値の加重平均で最終予測値を求め、これが自分のベストスコアとなった。今回のコンペはshakeのないコンペだったため、submitすることでモデルの性能を完全に把握することができたので、加重平均のウェイト調整にはleaderboardのスコアを使用した。

## 感想
---
コンペ終了後のフォーラムを見ても特徴量の作成はほとんど効果がなくTarget Encodingやpseudo labelingもあまり有効ではなかったというのは他の人や上位陣も同じだったようで、これは配布されたデータセットに原因があると考えられ、今回のコンペでは欠損値のないデータのわりには訓練データとテストデータの分布が非常に似ており、データセットに対してかなり人工的な細工がされているためデータセットからは新しい特徴量の作成が困難だったのだと思う。

また、特徴量に関して正解だったのはオリジナルデータと分布が大きく異なっていたbalanceとpdaysを除外することだったようで、自分のモデルではどちらの特徴量もFeature Importanceの高いモデルがあったので、除外することは考えなかったが、配布データはすでに加工されたデータという認識をもっと強く持っていればより広い視野を持ってコンペに望めたと思う。

コンペ終了10日前くらいにDeepTablesを用いた解法がフォーラムに投稿され、DeepTablesを使用したモデルを作成することによってスコアを改善できた人も多かったようだった。自分は就活と並行してコンペに参加していたので時間がなかったことや、DeepTablesを知らなかったことから、目だけ通してスルーしてしまったのが一番悔やまれる事だった。

そして、NNモデルが精度向上の鍵の1つだったことを考えると、NNモデルの改良に時間を割いてもよかった気もした。最終モデルとして採用するかどうか他のモデルと比較するために同じデータでモデルを作成していたが、daysにmin_max_encodingを行い、jobにはTarget Encodingを行うべきだった。OneHotEncodingによって大量に生まれた無意味な列を学習させてしまったことは最高にnoobだった。
